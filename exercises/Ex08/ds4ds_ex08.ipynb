{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ed1708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T09:38:29.647000+01:00",
     "start_time": "2024-12-02T08:38:28.583Z"
    }
   },
   "source": [
    "# DS4DS Homework Exercise Sheet 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416f699",
   "metadata": {},
   "source": [
    "**General Instructions:**\n",
    "\n",
    "- Collaborations between students during problem-solving phase on a discussion basis is OK\n",
    "- However: individual code programming and submissions per student are required\n",
    "- Code sharing is strictly prohibited\n",
    "- We will run checks for shared code, general plagiarism and AI-generated solutions\n",
    "- Any fraud attempt will lead to an auto fail of the entire course\n",
    "- Do not use any additional packages except for those provided in the task templates\n",
    "- Please use Julia Version 1.10.x to ensure compatibility\n",
    "- Please only write between the `#--- YOUR CODE STARTS HERE ---#` and `#--- YOUR CODE ENDS HERE ---#` comments\n",
    "- Please do not delete, add any cells or overwrite cells other than the solution cells (**Tip:** If you use a jupyerhub IDE, you should not be able to add or delete cells and write in the non-solution cells by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04941617",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19cc88d3162aa96633a762c833a1c972",
     "grade": false,
     "grade_id": "cell-a017067676c07b4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 1 - Automatic and Approximate Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef0d016",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "533ab8f643c99c39d58189c2cb7466e0",
     "grade": false,
     "grade_id": "cell-821030a5483bd276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "As some of you might have noticed with the last exercise sheet, \n",
    "manual differentiation of complicated functions tends to be error-prone and requires\n",
    "knowledge of the inner workings of said functions.\n",
    "Luckily, there are ways to avoid hand-crafted derivatives.\n",
    "If you have Julia functions performing tedious computations, you can use one of the many\n",
    "*Automatic Differentiation*  (AD) packages.\n",
    "A good AD package gives exact derivatives for differentiable functions, and oftentimes they \n",
    "employ clever tricks to do so fast and in a memory-efficient way.\n",
    "In case your function relies on external software (e.g., simulations), you could try to\n",
    "approximate the gradients.\n",
    "Traditionally, *Finite Differences* are the go-to approach for gradient approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572f483",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "659b6275c1f97c33268758df51044742",
     "grade": false,
     "grade_id": "cell-da3dcf2c18f5416d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "In this exercise, we are going to implement a finite difference scheme for multi-variate functions\n",
    "ourselves.\n",
    "Moreover, we take a look at the AD packages `FineteDiff`, `ForwardDiff` and `Zygote`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fe18c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3f593c5b8e359ec914271db493e2845",
     "grade": false,
     "grade_id": "cell-d9f102a46a3c11c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Manual Finite Differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692055b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad67ead84378230e177d8171a900f378",
     "grade": false,
     "grade_id": "cell-4a94c5c4702f5393",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We are interested in the (first-order) partial derivatives of $\\mathcal L\\colon ℝ^q \\to ℝ.$\n",
    "\n",
    "The [finite difference operator](https://en.wikipedia.org/wiki/Finite_difference) $Δ_h^{w_i}$ \n",
    "with grid-size $h > 0$ maps $\\mathcal L$ to $Δ_h^{w_i}[\\mathcal L]$, and we would like \n",
    "$Δ_h^{w_i}[\\mathcal L](\\mathbf w) \\approx ∂_{w_i} \\mathcal L(\\mathbf w)$.\n",
    "That is, the operator should approximate the partial derivative of $\\mathcal L$ with respect to $w_i$ \n",
    "at $\\mathbf w \\in ℝ^q$.\n",
    "If $\\mathcal L$ is two-times continuously differentiable, then the central finite difference scheme \n",
    "is of order 2, meaning that the error of the first-order derivatives is $\\mathcal O(h^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1e0e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db503ba7ff5230f46913446bced813bf",
     "grade": false,
     "grade_id": "cell-1c6e07122bc5a982",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "There are, of course, alternatives: forward and backward schemes, and schemes of higher \n",
    "accuracy or order, see [this table in the Wikipedia](https://en.wikipedia.org/wiki/Finite_difference_coefficient).\n",
    "For now, we want to stick with the central finite diffence scheme:\n",
    "$$\n",
    "Δ_h^{w_i}[\\mathcal L](\\mathbf w) = \\frac{\\mathcal L(\\mathbf w + h \\mathbf e_i) - \\mathcal L(\\mathbf w - h \\mathbf e_i)}{2 h}.\n",
    "$$\n",
    "The $i^{\\text{th}}$ unit vector $\\mathbf e_i$ is all zeros, except at position $i$, where it has entry $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6c932",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21c5568243816d55b7c0bfcac5809607",
     "grade": false,
     "grade_id": "cell-ae96cd79df98d438",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 1.a)\n",
    "Write a function `finite_diff_grad` that takes a function `func` ($\\mathcal L$), \n",
    "an input vector `w` and \n",
    "a scalar grid-size `h`, and returns the central finite difference gradient approximation \n",
    "$Δ_h[\\mathcal L](\\mathbf w) = [Δ_h^{w_1}[\\mathcal L](\\mathbf w), \\ldots, Δ_h^{w_q}[\\mathcal L](\\mathbf w)]^T \\in ℝ^q$. \\\n",
    "Do so by completing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c3ef17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:05.068000+01:00",
     "start_time": "2024-12-02T12:19:05.033Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a046c8a4ffe28ca09eece53b258463ab",
     "grade": false,
     "grade_id": "cell-bec4c71468437335",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finite_diff_grad"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    finite_diff_grad(func, w::AbstractVector{W}, h::H=0.0001f0) where {W<:Real, H<:Real}\n",
    "\n",
    "Return the finite difference gradient approximation of `func` at input `w`.\n",
    "\"\"\"\n",
    "function finite_diff_grad(func, w, h=0.0001)\n",
    "    @assert h > 0 \"Grid-size must be positive.\"\n",
    "\n",
    "    # pre-allocate solution array:\n",
    "    dw_func = zeros(length(w))\n",
    "    \n",
    "    # Note:\n",
    "    # `zero(w)` gives an array with same element type and length\n",
    "    # but above, `dw_func` is a `Vector{Float64}` to keep things simple\n",
    "    \n",
    "    # Now, fill `dw_func` to contain the finite difference approximations:    \n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    for i = 1:length(w)\n",
    "        e_i = zeros(length(w))\n",
    "        e_i[i] += 1\n",
    "        dw_func[i] = (func(w + h * e_i) - func(w - h * e_i)) / (2 * h)\n",
    "    end\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "    return dw_func\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ca99c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a1b6e58517a4447f7d268102a25e83c",
     "grade": false,
     "grade_id": "cell-cd1d0a227a8346ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can test the function in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbdf046e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:11.335000+01:00",
     "start_time": "2024-12-02T12:19:10.771Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a7428b8a9e228ea288772b74a632b68",
     "grade": true,
     "grade_id": "cell-21d7d55283ad807d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let # introduce a local scope to not accidentally pollute the global environment\n",
    "    func(w) = sum(w .^ 2)\n",
    "\n",
    "    w_one = ones(2)\n",
    "\n",
    "    dw_func = finite_diff_grad(func, w_one)\n",
    "    @assert length(dw_func) == length(w_one)\n",
    "    @assert dw_func ≈ [2, 2]\n",
    "    \n",
    "    # compute gradient of simple uni-variate function:\n",
    "    @assert finite_diff_grad( w -> w[1]^2, [1.0,] ) ≈ [2.0,]\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce28091",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b7d1b935c4c119e469fcc0423c8fe3d",
     "grade": false,
     "grade_id": "cell-8ca85bc231cb38cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 1.b)\n",
    "Investigate the accuracy of `finite_diff_grad` for different grid-sizes `h` on the function\n",
    "$$ \\mathcal L\\colon ℝ^q \\to ℝ, \\; \\mathcal L(\\mathbf w) = \\exp(w_1) + \\sum_{i=1}^q w^4_i,$$\n",
    "where $\\mathbf w = [w_1, …, w_q]^T$.\n",
    "\n",
    "Complete the code in the cell below to test the values $h\\in \\{10^{-1}, 10^{-2}, …, 10^{-10}\\}$.\n",
    "For each grid-size value $h$, store the error\n",
    "$$\n",
    "\\left\\| Δ_h[\\mathcal L](\\mathbf w) - \\nabla \\mathcal L(\\mathbf w) \\right\\|^2\n",
    "$$\n",
    "in `fd_grad_errors`. That is, besides the finite-difference approximation you also have to calculate the analytical derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5028924f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:17.837000+01:00",
     "start_time": "2024-12-02T12:19:17.321Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db117fc7c07ab71d9849a36094ce8f82",
     "grade": false,
     "grade_id": "cell-977ad28a3b7bdaea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_func(w) = exp(w[1]) + sum(w .^ 4)\n",
    "function grad_test_func(w)\n",
    "    global test_func\n",
    "    ## return the **true** gradient vector of `test_func` w.r.t. `w`\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    dw = zeros(length(w))\n",
    "    dw[1] = exp(w[1]) + 4 * w[1]^3\n",
    "    for i = 2:length(w)\n",
    "        dw[i] = 4 * w[i]^3\n",
    "    end\n",
    "    return dw\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end\n",
    "\n",
    "# grid sizes ``h`` to test:\n",
    "fd_grid_sizes = nothing\n",
    "# Change `fd_grid_sizes` to a vector conforming to the exercise statement:\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "fd_grid_sizes = zeros(10)\n",
    "for i = 1:10\n",
    "    fd_grid_sizes[i] = (10.)^(-i)\n",
    "end\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "# evaluation point, **don't** change\n",
    "w_fd = [π, 10, -ℯ / 20]\n",
    "\n",
    "# exact gradient vector `grad_test_func_exact` at `w_fd`\n",
    "grad_test_func_exact = nothing\n",
    "# Change `grad_test_func_exact` to hold the true gradient:\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "grad_test_func_exact = grad_test_func(w_fd)\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "\n",
    "# pre-allocate array to store the error values in\n",
    "fd_grad_errors = zero(fd_grid_sizes)\n",
    "for (i, h) in enumerate(fd_grid_sizes)\n",
    "    ## compute finite difference gradient and store error in `fd_grad_errors[i]`\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    fd_grad = finite_diff_grad(test_func, w_fd)\n",
    "    fd_grad_errors[i] = sqrt(sum((fd_grad .- grad_test_func_exact).^2))\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fd7bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:18.988000+01:00",
     "start_time": "2024-12-02T12:19:18.796Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e41052bbbb773ea6be89fbaa393f309d",
     "grade": true,
     "grade_id": "cell-3b7817dad2b08b0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert length(fd_grid_sizes) == 10\n",
    "@assert all(fd_grid_sizes[i] == 10.0^(-i) for i = eachindex(fd_grid_sizes))\n",
    "@assert grad_test_func(zeros(4)) == [1, 0, 0, 0]\n",
    "@assert sum(fd_grad_errors .^ 2) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd137059",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4dab41dffe4df580ce78f489dee98e0",
     "grade": false,
     "grade_id": "cell-745941105803a3f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can have a look at the error values by plotting them, for example with `Makie`.\n",
    "Usually, we would expect the error to decrease initially with the grid-size.\n",
    "But at some point, round-off errors will lead to an increase again.\n",
    "We can see this in this pre-made plot:\n",
    "![fd error plot](fd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0560da",
   "metadata": {},
   "source": [
    "### Exercise 1.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf8e89",
   "metadata": {},
   "source": [
    "#### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72416dbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e75175b1be9e7d2d4654aeec86513f38",
     "grade": false,
     "grade_id": "cell-d51adbc921c06c1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The Julia ecosystem provides many tools for [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "`Zygote` is used in the popular machine learning library [`Flux.jl`](https://fluxml.ai/Flux.jl/stable/) \n",
    "and implements reverse accumulation to compute loss function gradients.\n",
    "`ForwardDiff` is a forward-mode library and very robust. It works on functions acting on \n",
    "`Real` input.\n",
    "Lastly, `FiniteDiff` does as the name suggests and computes finite difference derivative\n",
    "approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63421f1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eac9b880978eea13fad47703c472afc",
     "grade": false,
     "grade_id": "cell-e4d482107e7d6cdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Flux and Lux are destined to move to Enzyme at some point in time.\n",
    "The project <a href=\"https://github.com/JuliaDiff/AbstractDifferentiation.jl\">AbstractDifferentiation</a>\n",
    "wants to provide a unified API for many differentiation packages, but lacks caching mechanisms\n",
    "and is thus not endorsed by SciML libraries (yet).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ef0676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:31.989000+01:00",
     "start_time": "2024-12-02T12:19:30.788Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0aa4f23337bf55c3826f4dd8386a9ed",
     "grade": false,
     "grade_id": "cell-b1ba7bb613ab49bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ForwardDiff as FD\n",
    "import FiniteDiff\n",
    "import Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ceb2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d74990e1e798791ae500660674c28155",
     "grade": false,
     "grade_id": "cell-e4d6cf8e04c55299",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Use all the above libraries to compute partial first-order derivatives of the scaled\n",
    "Gaussian RBF \n",
    "$$\n",
    "m(\\mathbf z; a, b, \\mathbf c) = b \\cdot \\exp\\left( -\\frac{\\| \\mathbf z - \\mathbf c \\|^2}{a^2} \\right).\n",
    "$$\n",
    "The function $m$ (like $m$odel) maps $\\mathbf z\\in ℝ^n$ to a scalar value.\n",
    "It is **c**entered at $\\mathbf c \\in ℝ^n$, and has shape parameter $a>0$, and scaling factor $b \\in ℝ$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a802176e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:34.868000+01:00",
     "start_time": "2024-12-02T12:19:34.465Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b058ff505cf711bd5dd76767bf40329d",
     "grade": false,
     "grade_id": "cell-b9d49e936eae357d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rbf (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rbf(z, a, b, c)\n",
    "    r_squared = sum((z .- c) .^ 2)\n",
    "    return b * exp(-r_squared / a^2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df7826",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbdbac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6f22b5c1ee77d939805889eaee89c90",
     "grade": false,
     "grade_id": "cell-68092e4786f251ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Use `FD.gradient`, `Zygote.gradient` and `FiniteDiff.finite_difference_derivative` to\n",
    "calculate the partial derivatives of $m$ with respect to $\\mathbf z$,\n",
    "$\\mathbf c$ and $a$ respectively:\n",
    "* `FD.gradient`: $\\partial m / \\partial \\mathbf z \\in \\mathbb{R}^2,$\n",
    "* `Zygote.gradient`: $\\partial m / \\partial \\mathbf c \\in \\mathbb{R}^2,$\n",
    "* `FiniteDiff.finite_difference_derivative`: $\\partial m / \\partial a \\in \\mathbb{R}.$\n",
    "\n",
    "To do so, you can use anonymous functions, e.g. `_z -> rbf(_z, c, a)`. \n",
    "**Always return a `Vector`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca69f8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:37.651000+01:00",
     "start_time": "2024-12-02T12:19:37.582Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96f1d4f617119d6a9c431804eba57afe",
     "grade": false,
     "grade_id": "cell-d99e87dd80b7e667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dz_rbf_ForwardDiff (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dz_rbf_ForwardDiff(z, a, b, c)\n",
    "    ## compute partial derivative of `rbf` with respect to `z` using `ForwardDiff`,\n",
    "    ## return a vector:\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    dz = FD.gradient(_z -> rbf(_z, a, b, c), z)\n",
    "    return dz\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424ae923",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:38.249000+01:00",
     "start_time": "2024-12-02T12:19:38.207Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52a0a850d3cbff4ae02af63390cc99bd",
     "grade": false,
     "grade_id": "cell-a6efb2ea3af9f0b1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dc_rbf_Zygote (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dc_rbf_Zygote(z, a, b, c)\n",
    "    ## compute partial derivative of `rbf` with respect to `c` using `Zygote`,\n",
    "    ## return a vector...\n",
    "\n",
    "    ## Take care: `Zygote.gradient` returns a tuple for each variable array you provide as argument!\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    dc = Zygote.gradient(_c -> rbf(z, a, b, _c), c)[1]\n",
    "    return dc\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8f5f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:38.836000+01:00",
     "start_time": "2024-12-02T12:19:38.795Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f42c00450d262b78a408639528b90f7",
     "grade": false,
     "grade_id": "cell-52f1790be26b0b7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "da_rbf_FiniteDiff (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function da_rbf_FiniteDiff(z, a, b, c)\n",
    "    ## FiniteDiff does not like the independent variable to be an Integer...\n",
    "    ## So we cast it it to `Float64`\n",
    "    a = Float64(a)\n",
    "    \n",
    "    ## compute partial derivative of `rbf` with respect to `a` using `FiniteDiff`,\n",
    "    ## return a vector...\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    da = FiniteDiff.finite_difference_derivative(_a -> rbf(z, _a, b, c), a)\n",
    "    da = [da]\n",
    "    return da\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340de2b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df2a07ec146eed9b931a8945fa5c12a2",
     "grade": false,
     "grade_id": "cell-1a938c07046810e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We should now be able to evaluate the partial gradients at \n",
    "$(\\mathbf z, \\mathbf c, a) = (\\mathbf 1, \\mathbf 0, 1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93ff44c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:42.622000+01:00",
     "start_time": "2024-12-02T12:19:40.445Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17ae2908b9e192aecb7cce055828d6a3",
     "grade": true,
     "grade_id": "cell-904df17cf70c2186",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let z = ones(2), c = zeros(2), a = 1, b = 1\n",
    "    ## evaluate partial gradient with respect to x\n",
    "    @assert dz_rbf_ForwardDiff(z, a, b, c) isa Vector\n",
    "    @assert length(dz_rbf_ForwardDiff(z, a, b, c)) == 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f78215f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:45.739000+01:00",
     "start_time": "2024-12-02T12:19:40.970Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7df5727f7dea80630152b2a1fbfdee40",
     "grade": true,
     "grade_id": "cell-c07b1dde789a9b35",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let z = ones(2), c = zeros(2), a = 1, b = 1\n",
    "    @assert dc_rbf_Zygote(z, a, b, c) isa Vector\n",
    "    @assert length(dc_rbf_Zygote(z, a, b, c)) == 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "632a8855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:19:57.658000+01:00",
     "start_time": "2024-12-02T12:19:57.457Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c8bc85d7494060fdfb92a26535cba93",
     "grade": true,
     "grade_id": "cell-fffea3b0ae598bf0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let z = ones(2), c = zeros(2), a = 1, b = 1\n",
    "    @assert da_rbf_FiniteDiff(z, a, b, c) isa Vector\n",
    "    @assert length(da_rbf_FiniteDiff(z, a, b, c)) == 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c688ac5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0495a075c5e8af81523cca421e458baf",
     "grade": false,
     "grade_id": "cell-e8f5f3998f062243",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa4abf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:04:22.054000+01:00",
     "start_time": "2024-12-02T10:04:22.030Z"
    }
   },
   "source": [
    "#### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b5f57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8179a991a1bf2fe2980c030a26a9fb60",
     "grade": false,
     "grade_id": "cell-3626459ce45c7656",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Just like on the last exercise sheet, we now compose a complete RBF approximation model \n",
    "$h \\colon ℝ^n \\to ℝ$\n",
    "as the sum of $N_\\text{RBF} \\in ℕ$ RBFs with different parameters\n",
    "$$\n",
    "h(\\mathbf z) = \n",
    "h(\\mathbf z; \\mathbf w)\n",
    "= \\sum_{i=1}^{N_{\\text{RBF}}} m(\\mathbf z; a_i, b_i, \\mathbf c_i).\n",
    "$$\n",
    "Here, $\\mathbf w$ is the complete (flattened) parameter vector of $h$ holding \n",
    "$a_i, b_i$ and $\\mathbf c_i$ for $i=1,…, N_{\\text{RBF}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d741ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:00.088000+01:00",
     "start_time": "2024-12-02T12:19:59.918Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0202e26185b854b6211fe627a7f0b0cc",
     "grade": false,
     "grade_id": "cell-44282cf715674ec5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h_rbf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "   h_rbf(z, w)\n",
    "\n",
    "Given a flattened parameter vector `w` (of suitable size), return the value\n",
    "of the complete RBF model.\n",
    "\"\"\"\n",
    "function h_rbf(z, w)\n",
    "    dim_z = length(z)\n",
    "    @assert dim_z > 0\n",
    "\n",
    "    num_rbf_params = dim_z + 2 # (1 center + shape param + coefficient) per RBF “kernel”\n",
    "    \n",
    "    ## check length of `w` vector\n",
    "    len_w = length(w)\n",
    "    @assert len_w >= num_rbf_params\n",
    "    @assert len_w % num_rbf_params == 0\n",
    "    num_rbfs = div(len_w, num_rbf_params)\n",
    "\n",
    "    ## unflatten and sum RBF kernels\n",
    "    h_val = 0\n",
    "    j = 1\n",
    "    for i = 1:num_rbfs\n",
    "        a = w[j]\n",
    "        b = w[j+1]\n",
    "        c = @view(w[j+2:j+1+dim_z])\n",
    "        h_val += rbf(z, a, b, c)\n",
    "        j += num_rbf_params\n",
    "    end\n",
    "    return h_val\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ef719",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eac115327a0cdf73aeae1cab49b59867",
     "grade": false,
     "grade_id": "cell-e83fee23109dfbb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Typically, models provided by some machine learning (like `Flux` or `Lux`), would not store \n",
    "their parameters in a flattened vector.\n",
    "Not only is the model implementation more intuitive that way, it also allows for optimized\n",
    "evaluation on large data sets.\n",
    "For example, Flux models return `Params` objects, that usually store arrays of varying dimensions, \n",
    "dependent on the model structure.\n",
    "The cool thing about `Zygote` is, that it keeps that structure when taking gradients.\n",
    "So the gradient of some loss function with respect to parameters that are stored in a matrix \n",
    "is a matrix, which enables convenient updating.\n",
    "<br/>\n",
    "Both libraries offer tools for flattening to use external optimizers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d2487",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4406a55e7abd9090b2c83775314cefa",
     "grade": false,
     "grade_id": "cell-abf831144201e5b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now assume that $\\mathbf Z \\in ℝ^{n \\times N}$ is a matrix, \n",
    "the columns of which hold feature vectors for labeled data.\n",
    "The labels are in $\\mathbf y\\in ℝ^{N}$.\n",
    "We want to use $h(•; \\mathbf w)$ to approximate the data and the arrays induce a loss function \n",
    "$$\n",
    "\\mathcal L(\\mathbf w) = \\mathcal L(\\mathbf w; \\mathbf Z, \\mathbf y) = \\frac{1}{N}\n",
    "\\sum_{j=1}^{N}\n",
    "(\\mathbf y_j - h(\\mathbf z_j, \\mathbf w)) ^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286927e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfc8d31f92644b14c150b459eb741347",
     "grade": false,
     "grade_id": "cell-9bd2683f9e72878e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Here is how to compute that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33e65a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:06.991000+01:00",
     "start_time": "2024-12-02T12:20:06.944Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5334985513772537a3ead3ea3994832",
     "grade": false,
     "grade_id": "cell-35bc1425db6a8ff5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rbf_mse (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rbf_mse(w, Z, y)\n",
    "    mse_val = 0\n",
    "    mse_divisor = 0\n",
    "    for (zj, yj) = zip(eachcol(Z), y)\n",
    "        mse_val += (h_rbf(zj, w) - yj)^2\n",
    "        mse_divisor += 1\n",
    "    end\n",
    "    mse_val /= mse_divisor\n",
    "    return mse_val\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81971a46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:05:57.221000+01:00",
     "start_time": "2024-12-02T10:05:57.193Z"
    }
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a9867",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f487dfb9159da5d79f54efae70f2a0e",
     "grade": false,
     "grade_id": "cell-736c306e6c05fcaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now it's your turn.\n",
    "\n",
    "Complete the cell below and use `Zygote.pullback` to compute the mean squared error **and the loss gradient**\n",
    "with respect to $\\mathbf w$ at the same time.\n",
    "The syntax is \n",
    "```julia\n",
    "result, back = Zygote.pullback( some_func, func_args )\n",
    "```\n",
    "and `back` is the **pullback function** that gives a tuple of gradient objects when called \n",
    "with seed `one(result)` (see lecture video on reverse-mode AD).\n",
    "\n",
    "Oftentimes, `some_func` is actually anonymous, e.g., to compute partial gradients only.\n",
    "In that case, it *can* be more convenient to use a `do`-block:\n",
    "```julia\n",
    "result, back = Zygote.pullback( func_arg_val1, func_arg_val2, … ) do func_arg_name1, func_arg_name2, …\n",
    "    # function body acting on `func_arg_name1` etc.\n",
    "    local_result\n",
    "end\n",
    "```\n",
    "Here is the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d72901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:08.950000+01:00",
     "start_time": "2024-12-02T12:20:08.907Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc43bde9e42a693801ea962961a055ac",
     "grade": false,
     "grade_id": "cell-677558e3fa05b627",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rbf_mse_and_grad (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rbf_mse_and_grad(w, Z, y)\n",
    "    # compute and return loss and loss gradient of `rbf_mse` with respect to w\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    result, back = Zygote.pullback(_w -> rbf_mse(_w, Z, y), w)\n",
    "    return result, back(one(result))[1]\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2f5d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecc5055bda67237ccc41cc7c735218c3",
     "grade": false,
     "grade_id": "cell-5d4fba3fecd54455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's test the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4a08a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:09.908000+01:00",
     "start_time": "2024-12-02T12:20:09.882Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c094f30a1221133d355b2b3e6df59cd",
     "grade": false,
     "grade_id": "cell-5483e86d33924f1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import Random   ## for reproducible pseudo random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206a703b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:26.042000+01:00",
     "start_time": "2024-12-02T12:20:10.392Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a74f7ba3c129b336752644bdc7770242",
     "grade": true,
     "grade_id": "cell-30bebf756bbc2bf7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let rng = Random.seed!(31415)\n",
    "    n = 3\n",
    "    N = 4\n",
    "\n",
    "    ## random training data\n",
    "    Z = rand(rng, n, N)\n",
    "    y = rand(rng, N)\n",
    "\n",
    "    ## build flattened parameter vector\n",
    "    num_rbfs = 10\n",
    "    w = rand(rng, (n + 2) * num_rbfs)\n",
    "\n",
    "    ## compute loss on random data\n",
    "    _L = rbf_mse(w, Z, y)\n",
    "\n",
    "    ## compute loss and loss gradient\n",
    "    L, dL = rbf_mse_and_grad(w, Z, y)\n",
    "\n",
    "    @assert L isa Number\n",
    "    @assert L == _L\n",
    "    @assert dL isa Vector\n",
    "\n",
    "    ## check consistency with ForwardDiff\n",
    "    _dL = FD.gradient(_w -> rbf_mse(_w, Z, y), w)\n",
    "    @assert _dL ≈ dL\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1a5e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be8958c88fc06f0ce553ce3b80c89ba5",
     "grade": false,
     "grade_id": "cell-f79cb4526fdc873f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 2 - Momentum Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e1b7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41d42878b186d675857edb8c99342787",
     "grade": false,
     "grade_id": "cell-659e352ed9e4a71e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "By extending the classical (stochastic) gradient descent update rule by a momentum term,\n",
    "we can sometimes observe an improved convergence rate.\n",
    "Polyak's Heavy Ball momentum is one of the best-known examples of this class of algorithms.\n",
    "The parameter update for a loss \n",
    "$\\mathcal L\\colon ℝ^q \\to ℝ, \\mathbf w \\mapsto \\mathcal L(\\mathbf w)$ \n",
    "in iteration $k\\in ℕ_0$ is \n",
    "$$\n",
    "\\mathbf w^{(k+1)}\n",
    "\\leftarrow\n",
    "\\mathbf w^{(k)}\n",
    "-\n",
    "α \\nabla \\mathcal L(\\mathbf w^{(k)})\n",
    "+ \n",
    "κ\n",
    "(\\mathbf w^{(k)} - \\mathbf w^{(k-1)}).\n",
    "$$\n",
    "We assume to start with $\\mathbf w^{(0)}$.\n",
    "The algorithm is instantiated with $\\mathbf w^{(-1)} = \\mathbf w^{(0)}$, i.e., without momentum in the first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe872604",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6186cd8a6b466f266e504ea651a1ed08",
     "grade": false,
     "grade_id": "cell-744a5fe544dcda24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "To test our algorithm, we are again considering the _Rosenbrock function_:\n",
    "$$\n",
    "\\mathcal L(\\mathbf w) = (a - w_1)^2 + b(w_2 - w_1^2)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07badba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:26.057000+01:00",
     "start_time": "2024-12-02T12:20:14.808Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b723b60092bc2c937bc283558d41f5c8",
     "grade": false,
     "grade_id": "cell-a9790be3b7104956",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rosenbrock_2D (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rosenbrock_2D(w; a=1, b=100)\n",
    "    return (a - w[1])^2 + b * (w[2] - w[1]^2)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe991780",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55bc1cd252a57ac6cf38fdac5a75e1b9",
     "grade": false,
     "grade_id": "cell-78ab38c08e4f9981",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 2a)\n",
    "Implement the **exact** gradient of the Rosenbrock function. You can compute it by hand or use `ForwardDiff` or `Zygote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e3881c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:26.073000+01:00",
     "start_time": "2024-12-02T12:20:15.744Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad0cb37761b4f324ac71785b37086943",
     "grade": false,
     "grade_id": "cell-9c19c6bae6e5d27f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dw_rosenbrock_2D (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function dw_rosenbrock_2D(w; a=1, b=100)\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    dw1 = 4 * b * w[1]^3 + 2 * w[1] - 4 * b * w[2] * w[1] - 2 * a\n",
    "    dw2 = 2 * b * (w[2] - w[1]^2)\n",
    "    dw = [dw1, dw2]\n",
    "    return dw\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a81d5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9295a425f797f8491cd1798e34f82d1",
     "grade": false,
     "grade_id": "cell-b9cb438ab2ca4b42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The global optimum is known to be $\\mathbf x = [a, a^2]^\\top$ and we can check for consistency,\n",
    "if the value is zero and the gradient vanishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3006baa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:27.688000+01:00",
     "start_time": "2024-12-02T12:20:16.804Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71405936de5c1cb9a1910da5bb614c6a",
     "grade": true,
     "grade_id": "cell-9864723c0d510dcc",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let # introduce a local scope to not pollute the global scope by accident \n",
    "    L = rosenbrock_2D([1, 1])\n",
    "    @assert iszero(L)\n",
    "    dL = dw_rosenbrock_2D([1, 1])\n",
    "    @assert all(iszero.(dL))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63133ad1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b794ab92cd604165b09491c7eceacd05",
     "grade": false,
     "grade_id": "cell-c29cf92397bc039e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec406c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "753988012b52c6bdfd19cc53a796d66d",
     "grade": false,
     "grade_id": "cell-cfd21900cd86d357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We now want to implement a training algorithm `momentum_descent` with fixed\n",
    "meta-parameters.\n",
    "Besides the meta parameters, the function is provided with functions\n",
    "`loss`, `dw_loss` and initial parameters `w`.\n",
    "\n",
    "Assume `loss` to return a scalar loss value when called as `loss(w)`, and `dw_loss`\n",
    "to return a loss gradient vector.\n",
    "\n",
    "Fill in the cell below according to the comments to finalize the algorithm implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c77d1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf w^{(k+1)}\n",
    "\\leftarrow\n",
    "\\mathbf w^{(k)}\n",
    "-\n",
    "α \\nabla \\mathcal L(\\mathbf w^{(k)})\n",
    "+ \n",
    "κ\n",
    "(\\mathbf w^{(k)} - \\mathbf w^{(k-1)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1478d576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:20:51.025000+01:00",
     "start_time": "2024-12-02T12:20:50.978Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e29c461325d9c17333017bfa6cb57a0",
     "grade": false,
     "grade_id": "cell-ce7df560f40f31be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "momentum_descent (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function momentum_descent(\n",
    "    loss,           # loss function\n",
    "    dw_loss,        # gradient function\n",
    "    w;              # initial model parameters\n",
    "    num_iter=10,    # number of iterations              \n",
    "    alpha=0.1f0,    # gradient stepsize\n",
    "    kappa=0.1f0,    # momentum factor\n",
    ")\n",
    "\n",
    "    wk = copy(w)  # do not modify the initial vector!\n",
    "\n",
    "    ## It is good practice to preallocate memory before any loop.\n",
    "    ## Hence, initialize an object `w_prev` for the previous parameters and set \n",
    "    ## the values according to the formula for the Heavy Ball scheme.\n",
    "    ## Additionally, allocate a vector `momentum` for the momentum term,\n",
    "    ## i.e., the difference between two consecutive weight vectors.\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    w_prev = zeros(size(w))\n",
    "    momentum = zeros(size(w))\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "    ## Finally, do the iterations by completing the code in the for loop below.\n",
    "    ## Right now, `k` can be thought to equal 0.\n",
    "    ## At the end of each loop iteration, `wk` should be updated to be valid for `k`.\n",
    "    ## E.g., if `num_iter==1`, we have 1 iteration, update `wk`, and return ``\\mathbf w^{(1)}``.\n",
    "    for k = 1:num_iter\n",
    "        ## 1) Obtain a loss gradient for the current `wk`.\n",
    "        ## 2) Modify `momentum`, `w_prev` and `wk` according to the Heavy Ball formulas.\n",
    "        #--- YOUR CODE STARTS HERE ---#\n",
    "        momentum = wk - w_prev\n",
    "        w_prev = wk\n",
    "        wk = wk - alpha .* dw_loss(wk) + kappa .* momentum\n",
    "        #--- YOUR CODE ENDS HERE ---#\n",
    "    end\n",
    "\n",
    "    ## return last parameters\n",
    "    return wk\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8fd7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "722137410ac1c6865de316396fb9131d",
     "grade": false,
     "grade_id": "cell-d613240128e3623a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 2c)\n",
    "\n",
    "Minimize the Rosenbrock function with $a=1$ and $b=100$ using your momentum algorithm. Use the same method to compare the results to those of the standard gradient descent (think on how to choose the hyper-parameters to achieve this!)\n",
    "\n",
    "Perform 100 iterations, starting at `w0_rb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a55e7645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:12.292000+01:00",
     "start_time": "2024-12-02T12:21:11.442Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc1c8efdf2749e4c255f12900209470a",
     "grade": false,
     "grade_id": "cell-204af9b0b5ceaa3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_iter_rb = 100\n",
    "\n",
    "## some initial guess:\n",
    "w0_rb = [-π / 2, ℯ / 4] # don't change!\n",
    "\n",
    "## obtain loss function from `rosenbrock_2D` for `a=1` and `b=100` and assign it to `loss_rb`.\n",
    "## `loss_rb` will be the first argument for `momentum_descent`\n",
    "loss_rb = nothing    # redefine below\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "loss_rb = _w -> rosenbrock_2D(_w)\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "## likewise, obtain loss gradient function `dw_loss_rb` from `rosenbrock_2D` for `a=1` and `b=100`\n",
    "## `dw_loss_rb` will be the second argument for `momentum_descent`\n",
    "dw_loss_rb = nothing   # redefine below\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "dw_loss_rb = _w -> dw_rosenbrock_2D(_w)\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "## here you can see how we do 100 iterations of momentum descent:\n",
    "alpha_momentum = 1e-3\n",
    "kappa_momentum = 0.7\n",
    "wopt_rb_momentum = momentum_descent(\n",
    "    loss_rb, dw_loss_rb, w0_rb;\n",
    "    num_iter=num_iter_rb, alpha=alpha_momentum, kappa=kappa_momentum\n",
    ")\n",
    "\n",
    "## exercise: compare against steepest descent!\n",
    "alpha_sd = alpha_momentum\n",
    "kappa_sd = nothing  # redefine below\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "kappa_sd = 0\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "wopt_rb_sd = momentum_descent(\n",
    "    loss_rb, dw_loss_rb, w0_rb;\n",
    "    num_iter=num_iter_rb, alpha=alpha_sd, kappa=kappa_sd\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d48d6d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63e2ba32bc87258524379558536a9688",
     "grade": false,
     "grade_id": "cell-739b82f85fab16ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let's check your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9765329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:12.573000+01:00",
     "start_time": "2024-12-02T12:21:12.554Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7a2727c5b77f43ff0b004aa0d7fbba1",
     "grade": true,
     "grade_id": "cell-274861887c31dda2",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert kappa_sd isa Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcf4d6ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:13.051000+01:00",
     "start_time": "2024-12-02T12:21:12.991Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0c8c706084833a3c92061b7f4f733b6",
     "grade": true,
     "grade_id": "cell-7861bb85d3f17b03",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert wopt_rb_momentum isa Vector\n",
    "@assert wopt_rb_sd isa Vector\n",
    "\n",
    "loss_rb(w0_rb) ≈ 326.24283461518615\n",
    "dw_loss_rb(w0_rb) ≈ [-1128.4687155349027; -357.56612863151565]\n",
    "\n",
    "# After iteration, the loss has hopefully been reduced:\n",
    "@assert loss_rb(wopt_rb_sd) < loss_rb(w0_rb)\n",
    "@assert loss_rb(wopt_rb_momentum) < loss_rb(w0_rb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891030cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d833f33918d14d8e2a4fbcaa9c88f61f",
     "grade": false,
     "grade_id": "cell-e5ad99c006af4e9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If your implementation works as it should, the solution trajectories look something like this:\n",
    "![momentum descent with rosenbrock](momentum_rb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4b6ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "085b9b6d2cdc543513a3d961b845b08e",
     "grade": false,
     "grade_id": "cell-b4051006928df375",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89b89f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f54656a08ee77519a310d05f20da5238",
     "grade": false,
     "grade_id": "cell-f6f24ae3fe363026",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Finally, in the cell below, test several configurations of the descent algorithm for \n",
    "optimization of the Rosenbrock function $\\mathcal L$ with $a=1$ and $b=100$.\n",
    "\n",
    "For `w0_rb` from above, perform 50 iterations of momentum descent for the \n",
    "meta-parameters $(α, κ)$ in `alpha_kappa_configs`.\n",
    "Among those tuples, determine the tuple `(alpha_best, kappa_best)` that achieves the smallest\n",
    "function value after 50 iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90abe11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:16.387000+01:00",
     "start_time": "2024-12-02T12:21:15.955Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ef9490f9cbebf6f88931c1f146b3a1b",
     "grade": false,
     "grade_id": "cell-2e1852ac2c9a5a22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## these are the meta-parameters to investigate\n",
    "alpha_kappa_configs = Iterators.product((1e-4, 1e-5, 1e-6, 1e-7), (0.6, 0.2, 0.1, 1e-3, 1e-4, 1e-6))\n",
    "\n",
    "## at the end of this cell block, you should have assigned fitting values to this tuple:\n",
    "(alpha_best, kappa_best) = (-1.0, -1.0)\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "best_loss = 10000\n",
    "for (alpha, kappa) in alpha_kappa_configs\n",
    "    wopt = momentum_descent(loss_rb, dw_loss_rb, w0_rb;\n",
    "    num_iter=50, alpha=alpha, kappa=kappa);\n",
    "    if rosenbrock_2D(wopt) < best_loss\n",
    "        alpha_best = alpha\n",
    "        kappa_best = kappa\n",
    "    end\n",
    "end\n",
    "#--- YOUR CODE ENDS HERE ---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21448057",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:16.396000+01:00",
     "start_time": "2024-12-02T12:21:16.305Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0978e4cc4aa57295c2d622a8800b878f",
     "grade": true,
     "grade_id": "cell-cf81248728ec7def",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert alpha_best isa Real\n",
    "@assert kappa_best isa Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc96bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ca92bfe6ea47a432263934dbebff9c5",
     "grade": false,
     "grade_id": "cell-95082357fc060a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Exercise 3 - Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc616b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79f232efc07cadeb9c8c037a1813640e",
     "grade": false,
     "grade_id": "cell-4d7283cda136e5d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Newton's method is a root finding algorithm.\n",
    "Consider the function $\\mathbf m\\colon ℝ^q \\to ℝ^q$.\n",
    "Then Newton's method tries to find $\\mathbf w^* \\in ℝ^q$ with $\\mathbf m(\\mathbf w^*) = \\mathbf 0$.\n",
    "The update rule is \n",
    "$$\n",
    "\\mathbf w_{k+1} = \\mathbf w_k - \\left(\\nabla \\mathbf m(\\mathbf w_k)\\right)^{-1} \\mathbf m(\\mathbf w_k),\n",
    "\\quad \n",
    "k\\in \\mathbb N_0,\n",
    "$$\n",
    "where $\\nabla \\mathbf m(\\mathbf w_k)$ is the Jacobian of $\\mathbf m$ at $\\mathbf w_k$.\n",
    "\n",
    "Instead of computing the inverse in the right-most term, rather substitute it by $\\mathbf v$ and solve a linear equation system\n",
    "$$\n",
    "\\nabla \\mathbf m(\\mathbf w_k) \\mathbf v = - \\mathbf m(\\mathbf w_k)\n",
    "$$\n",
    "to obtain $\\mathbf w_{k+1} = \\mathbf v + \\mathbf w_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0cd31f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c158a8b28bc93cccb5cee0ba161c23ca",
     "grade": false,
     "grade_id": "cell-74c6a7991d785a0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 3a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0712cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d46c88d70fc2078cb3a75cf420971ed",
     "grade": false,
     "grade_id": "cell-6c737008af60569f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "In the cell below, use the formulas from above to implement Newton's root finding algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "800a81b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:18.433000+01:00",
     "start_time": "2024-12-02T12:21:18.383Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "662421ac7b7396ae4c66c5945d585ac4",
     "grade": false,
     "grade_id": "cell-b6845bf8651a9b70",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton_opt (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function newton_opt(\n",
    "    func,       # the function ``m`` from above\n",
    "    jac_func,   # a function to obtain the jacobian of ``m``\n",
    "    w0          # initial guess for the root\n",
    "    ;\n",
    "    num_iter=10,    # number of iterations\n",
    "    abs_tol=0,      # absolute stopping tolerance\n",
    ")\n",
    "    ## initialize `wk`\n",
    "    wk = copy(w0)\n",
    "    \n",
    "    for k=1:num_iter\n",
    "        ## evaluate `func` at `wk` and store results in `mk`\n",
    "        #--- YOUR CODE STARTS HERE ---#\n",
    "        mk = func(wk)\n",
    "        #--- YOUR CODE ENDS HERE ---#\n",
    "        \n",
    "        ## if norm squared of `wk` is <= `abs_tol`, then break:\n",
    "        #--- YOUR CODE STARTS HERE ---#\n",
    "        if sqrt(sum(wk.^2)) <= abs_tol\n",
    "            break\n",
    "        end\n",
    "        #--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "        ## compute jacobian at `wk` and store result in `d_mk`\n",
    "        ## then set `v` to solve `d_mk * v = -mk`\n",
    "        #--- YOUR CODE STARTS HERE ---#\n",
    "        d_mk = jac_func(wk)\n",
    "        v = d_mk \\ -mk\n",
    "        #--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "        ## finally, update `wk` with `v` to store values for iteration `k+1`\n",
    "        #--- YOUR CODE STARTS HERE ---#\n",
    "        wk += v\n",
    "        #--- YOUR CODE ENDS HERE ---#\n",
    "    end\n",
    "\n",
    "    return wk\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb846e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "846274f6c126d7e224e570802e707b43",
     "grade": false,
     "grade_id": "cell-e106e3f30d6a4784",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Let us test your algorithm with a simple example. \n",
    "The Jacobian of $\\mathbf m( w_1, w_2 ) = [w_1^2, w_2^2]$ is \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    2w_1 & 0 \\\\\n",
    "    0 & 2w_2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Newton's algorithm should approximate $\\mathbf w^* = [0, 0]^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb38b9c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:21.167000+01:00",
     "start_time": "2024-12-02T12:21:19.282Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba206cb506889e1584305ce1b100b753",
     "grade": true,
     "grade_id": "cell-b3ae6d9a5efa2e8b",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "let\n",
    "    ## test function:\n",
    "    m = w -> w .^ 2\n",
    "    ## jacobian:\n",
    "    dw_m = w -> [\n",
    "        2*w[1] 0;\n",
    "        0 2*w[2]\n",
    "    ]\n",
    "    \n",
    "    ## inital guess\n",
    "    w0 = [1.0, -3.0]\n",
    "    ## call newton root finder:\n",
    "    wopt = newton_opt(m, dw_m, w0; num_iter=20, abs_tol=0)\n",
    "    @assert sum(m(wopt) .^ 2) <= 1e-10\n",
    "\n",
    "    ## test stopping criterion:\n",
    "    _wopt = newton_opt(m, dw_m, w0; num_iter=20, abs_tol=1e-3)\n",
    "    @assert sum(m(_wopt) .^ 2) <= 1e-3\n",
    "    @assert m(wopt) <= m(wopt)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b35874",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8acd2bf23b878e6285f1d3a29d607a55",
     "grade": false,
     "grade_id": "cell-4fef13b34d4adda0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Exercise 3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b4242",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "608f87f8499fc0134741dcf1314b3f57",
     "grade": false,
     "grade_id": "cell-4d1b29bd2b243ca3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We want to use Newton's method to **minimize** the Rosenbrock function.\n",
    "As an optimization algorithm for $\\mathcal L\\colon ℝ^q \\to ℝ$, Newton's root finding scheme is applied to the function \n",
    "$$\n",
    "\\mathbf w \\mapsto \\nabla \\mathcal L(\\mathbf w)\n",
    "$$\n",
    "Hence, we need the Jacobian of the gradient of the Rosenbrock function for `newton_opt`, which is the _Hessian matrix_.\n",
    "Complete the cell below to return this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7ac0a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:21.184000+01:00",
     "start_time": "2024-12-02T12:21:20.435Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7eab372630441ec375d439377f5aa92",
     "grade": false,
     "grade_id": "cell-1b376634e23234cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hess_rosenbrock_2D (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function hess_rosenbrock_2D(w; a=1, b=100)\n",
    "    #--- YOUR CODE STARTS HERE ---#\n",
    "    hess = Zygote.hessian(_w -> rosenbrock_2D(_w; a, b), w)\n",
    "    return hess\n",
    "    #--- YOUR CODE ENDS HERE ---#\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9517c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:25.766000+01:00",
     "start_time": "2024-12-02T12:21:21.568Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c375f648929c092f462b5d667769d6",
     "grade": true,
     "grade_id": "cell-9c472047fad75c0a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert hess_rosenbrock_2D(ones(2)) ≈ [\n",
    "    802 -400;\n",
    "    -400 200\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7d77e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfe8bf91d6afab6619d92f1eac68865b",
     "grade": false,
     "grade_id": "cell-9e608083d61b5e88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now apply `newton_opt` to the problem of minimizing the Rosenbrock function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35821871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:25.883000+01:00",
     "start_time": "2024-12-02T12:21:23.107Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bfe71b49cff3be0f9582aba7cb82296",
     "grade": false,
     "grade_id": "cell-8e067d0609dec532",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_rb = nothing\n",
    "## below, override `m_rb` to a suitable function, such that Newton's method optimizes the\n",
    "## Rosenbrock function with parameters `a = 2, b = 100`. \n",
    "## `m_rb` is given to `newton_opt` as the first argument.\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "m_rb = _w -> dw_rosenbrock_2D(_w, a=2, b=100)\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "dw_m_rb = nothing\n",
    "## below, set `dw_m_rb` to a suitable function that is used as the second argument for `newton_opt`:\n",
    "#--- YOUR CODE STARTS HERE ---#\n",
    "dw_m_rb = _w -> hess_rosenbrock_2D(_w, a=2, b=100)\n",
    "#--- YOUR CODE ENDS HERE ---#\n",
    "\n",
    "## let's actually call the algorithm:\n",
    "w_opt_newton = newton_opt(m_rb, dw_m_rb, w0_rb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6847d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:21:26.364000+01:00",
     "start_time": "2024-12-02T12:21:26.343Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "656a72f4b3dc1cc7084231020d6f869a",
     "grade": true,
     "grade_id": "cell-0a00c613a42f0db5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@assert w_opt_newton isa Vector\n",
    "@assert length(w_opt_newton) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57fed4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc5e73f7e0a0c62acf5e11a648bcb65b",
     "grade": false,
     "grade_id": "cell-a3468ace8a0969ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now, the solution trajectory should reach the optimum quickly:\n",
    "![newton iterations](newton.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.1",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
